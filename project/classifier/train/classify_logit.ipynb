{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "import csv\n",
    "import copy\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "from feature import Feature\n",
    "import sys\n",
    "sys.dont_write_bytecode = True\n",
    "sys.path.append('../')\n",
    "from datatool.preprocess import  Preprocessor\n",
    "from datatool.maneger import DataManager"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/usr/lib/python3/dist-packages/requests/__init__.py:80: RequestsDependencyWarning: urllib3 (1.26.5) or chardet (3.0.4) doesn't match a supported version!\n",
      "  RequestsDependencyWarning)\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "path = \"../corpus/data_augment.csv\""
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "data = []\n",
    "with open(path) as f:\n",
    "    reader = csv.reader(f)\n",
    "    for row in reader:\n",
    "        data.append(row)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "def make_classes_dict(data):\n",
    "    classes_dict = dict()\n",
    "    for row in data:\n",
    "        c = row[1]\n",
    "        if c not in classes_dict:\n",
    "            classes_dict[c] = len(classes_dict)\n",
    "    return classes_dict"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "def class_filter(data, remain_classes:list):\n",
    "    new_data = []\n",
    "    for row in data:\n",
    "        c = row[1]\n",
    "        if c in remain_classes:\n",
    "            new_data.append(row)\n",
    "    return new_data"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "classes_dict_ = make_classes_dict(data)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "remain_classes = \"how what when where who why YN plain\".split()\n",
    "data_n = class_filter(data, remain_classes)\n",
    "classes_dict = dict(zip(remain_classes, list(range(len(remain_classes)))))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "def extract_X_y(data, classes_dict):\n",
    "    X = []\n",
    "    y = []\n",
    "    for d in data:\n",
    "        X.append(d[0])\n",
    "        y.append(classes_dict[d[1]])\n",
    "    return X, y"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "import random\n",
    "def extract_X_y_limit(data, classes_dict:dict, limit=10):\n",
    "    data_ = random.sample(data, len(data))\n",
    "    X = []\n",
    "    y = []\n",
    "    each_len = dict(zip(classes_dict.keys(), [0]*len(classes_dict)))\n",
    "    for d in data_:\n",
    "        if each_len[d[1]] <= limit:\n",
    "            X.append(d[0])\n",
    "            y.append(classes_dict[d[1]])\n",
    "            each_len[d[1]] += 1\n",
    "    return X, y"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "# X, y = extract_X_y(data_n, classes_dict)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "source": [
    "X, y = extract_X_y_limit(data_n, classes_dict)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "source": [
    "len(X)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "88"
      ]
     },
     "metadata": {},
     "execution_count": 13
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "source": [
    "X_train_str, X_test_str, y_train, y_test = train_test_split(X, y, test_size=0.30, random_state=5)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "source": [
    "F = Feature()\n",
    "F.set_preprocessor(Preprocessor())\n",
    "F.make_features(X_train_str)\n",
    "\n",
    "X_train = []\n",
    "X_test = []\n",
    "for i, x_t_str in enumerate( X_train_str ):\n",
    "    x = F.featurization(x_t_str)\n",
    "    X_train.append(x)\n",
    "for i, x_t_str in enumerate( X_test_str ):\n",
    "    x = F.featurization(x_t_str)\n",
    "    X_test.append(x)\n",
    "X_train = np.array(X_train)\n",
    "X_test = np.array(X_test)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "300\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "source": [
    "F_path = \"../X_y_data/\"\n",
    "F_name = \"typeClassify_F2.pickle\"\n",
    "import dill\n",
    "import pickle\n",
    "# featureM = DataManager(F_path, format_=\"dill\")\n",
    "with open(F_path+F_name, \"wb\") as f:\n",
    "    pickle.dump(F, f)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "source": [
    "lr = LogisticRegression(solver='sag', max_iter=100)\n",
    "lr.fit(X_train, y_train)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/home/yamada/.local/lib/python3.6/site-packages/sklearn/linear_model/_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "LogisticRegression(solver='sag')"
      ]
     },
     "metadata": {},
     "execution_count": 17
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "source": [
    "y_pred = lr.predict(X_test)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "source": [
    "print(classes_dict)\n",
    "dis = 30\n",
    "for y_p, x_s in zip(y_pred[:dis], X_test_str[:dis]):\n",
    "    print(\"{0} : {1}\".format(y_p, x_s))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "{'how': 0, 'what': 1, 'when': 2, 'where': 3, 'who': 4, 'why': 5, 'YN': 6, 'plain': 7}\n",
      "6 : パイを3つもらおう。\n",
      "4 : 取り残されることの恐怖から、つまりFOMOから、通知やメッセージに絶えず反応する循環に陥っているのです。\n",
      "0 : どうしてわかったの？\n",
      "2 : あなたの名前は何ですか？\n",
      "2 : どんな所がお好みですか？\n",
      "3 : きみがぼくのために作ってくれたの？\n",
      "0 : ハワードさん、タクシー呼びましょうか？\n",
      "0 : あなたたち、なぜ笑っているの？\n",
      "2 : いちばん好きなクリスマス・ソングは何ですか？\n",
      "3 : いつお店に行ったの？\n",
      "0 : あなたたちは、どうしてスナップドラゴンの手下になっているの？\n",
      "4 : どういうこと？\n",
      "3 : その店はどこにあるの？\n",
      "4 : ウィンター山脈では一年中雪が降る。\n",
      "3 : ホームセンターはどこなの？\n",
      "4 : 確認願えますか？\n",
      "1 : これは正確には何でしたっけ？\n",
      "2 : いつ建てられたのですか？\n",
      "4 : あの女の子はだれ？\n",
      "1 : 何歳なの？\n",
      "5 : わたしたち、ここで1年くらい前に出会ったんだよね、オリバー。\n",
      "4 : どこでフェイスペインティングを習ったの、レイチェル?\n",
      "3 : 誰がそのおいしそうなチョコレートケーキを作ったの？\n",
      "2 : 和太鼓の稽古はないの？\n",
      "5 : マリーゴールド畑に入ると、そう簡単には出られないのです。\n",
      "0 : 俳優たちの顔の表情を見た？\n",
      "1 : 音楽が好きなの？\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "source": [
    "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score\n",
    "print('confusion matrix = \\n', confusion_matrix(y_true=y_test, y_pred=y_pred))\n",
    "print('accuracy = ', accuracy_score(y_true=y_test, y_pred=y_pred))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "confusion matrix = \n",
      " [[2 1 0 0 0 0 0 0]\n",
      " [0 1 3 0 1 0 0 0]\n",
      " [0 0 1 1 0 0 0 0]\n",
      " [0 0 0 2 1 0 0 0]\n",
      " [0 0 0 1 1 0 0 0]\n",
      " [2 0 0 0 0 0 0 0]\n",
      " [1 1 1 1 1 0 0 0]\n",
      " [0 0 0 0 2 2 1 0]]\n",
      "accuracy =  0.25925925925925924\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "source": [
    "model_path = \"../models/\"\n",
    "model_name = \"typeClassify_M2.pickle\"\n",
    "modelM = DataManager(model_path)\n",
    "print(model_name)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "typeClassify_M2.pickle\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "source": [
    "modelM.save_data(model_name, lr)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "success save : ../models/typeClassify_M2.pickle\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.6.9",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.6.9 64-bit"
  },
  "interpreter": {
   "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}