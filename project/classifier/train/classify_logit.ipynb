{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "import csv\n",
    "import copy\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "from feature import Feature\n",
    "import sys\n",
    "sys.dont_write_bytecode = True\n",
    "sys.path.append('../')\n",
    "from datatool.preprocess import  Preprocessor\n",
    "from datatool.maneger import DataManager"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/usr/lib/python3/dist-packages/requests/__init__.py:80: RequestsDependencyWarning: urllib3 (1.26.5) or chardet (3.0.4) doesn't match a supported version!\n",
      "  RequestsDependencyWarning)\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "path = \"../corpus/data_augment.csv\""
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "data = []\n",
    "with open(path) as f:\n",
    "    reader = csv.reader(f)\n",
    "    for row in reader:\n",
    "        data.append(row)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "def make_classes_dict(data):\n",
    "    classes_dict = dict()\n",
    "    for row in data:\n",
    "        c = row[1]\n",
    "        if c not in classes_dict:\n",
    "            classes_dict[c] = len(classes_dict)\n",
    "    return classes_dict"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "def class_filter(data, remain_classes:list):\n",
    "    new_data = []\n",
    "    for row in data:\n",
    "        c = row[1]\n",
    "        if c in remain_classes:\n",
    "            new_data.append(row)\n",
    "    return new_data"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "classes_dict_ = make_classes_dict(data)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "remain_classes = \"how what when where who why YN plain\".split()\n",
    "data_n = class_filter(data, remain_classes)\n",
    "classes_dict = dict(zip(remain_classes, list(range(len(remain_classes)))))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "def extract_X_y(data, classes_dict):\n",
    "    X = []\n",
    "    y = []\n",
    "    for d in data:\n",
    "        X.append(d[0])\n",
    "        y.append(classes_dict[d[1]])\n",
    "    return X, y"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "import random\n",
    "def extract_X_y_limit(data, classes_dict:dict, limit=400):\n",
    "    data_ = random.sample(data, len(data))\n",
    "    X = []\n",
    "    y = []\n",
    "    each_len = dict(zip(classes_dict.keys(), [0]*len(classes_dict)))\n",
    "    for d in data_:\n",
    "        if each_len[d[1]] <= limit:\n",
    "            X.append(d[0])\n",
    "            y.append(classes_dict[d[1]])\n",
    "            each_len[d[1]] += 1\n",
    "    return X, y"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "# X, y = extract_X_y(data_n, classes_dict)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "source": [
    "X, y = extract_X_y_limit(data_n, classes_dict)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "source": [
    "len(X)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "3208"
      ]
     },
     "metadata": {},
     "execution_count": 13
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "source": [
    "X_train_str, X_test_str, y_train, y_test = train_test_split(X, y, test_size=0.30, random_state=5)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "source": [
    "F = Feature()\n",
    "F.set_preprocessor(Preprocessor())\n",
    "F.make_features(X_train_str)\n",
    "\n",
    "X_train = []\n",
    "X_test = []\n",
    "for i, x_t_str in enumerate( X_train_str ):\n",
    "    x = F.featurization(x_t_str)\n",
    "    X_train.append(x)\n",
    "for i, x_t_str in enumerate( X_test_str ):\n",
    "    x = F.featurization(x_t_str)\n",
    "    X_test.append(x)\n",
    "X_train = np.array(X_train)\n",
    "X_test = np.array(X_test)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "source": [
    "F_path = \"../X_y_data/\"\n",
    "F_name = \"typeClassify_F2.pickle\"\n",
    "import dill\n",
    "import pickle\n",
    "# featureM = DataManager(F_path, format_=\"dill\")\n",
    "with open(F_path+F_name, \"wb\") as f:\n",
    "    pickle.dump(F, f)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "source": [
    "lr = LogisticRegression(solver='sag', max_iter=100)\n",
    "lr.fit(X_train, y_train)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/home/yamada/.local/lib/python3.6/site-packages/sklearn/linear_model/_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "LogisticRegression(solver='sag')"
      ]
     },
     "metadata": {},
     "execution_count": 17
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "source": [
    "y_pred = lr.predict(X_test)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "source": [
    "print(classes_dict)\n",
    "dis = 30\n",
    "for y_p, x_s in zip(y_pred[:dis], X_test_str[:dis]):\n",
    "    print(\"{0} : {1}\".format(y_p, x_s))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "{'how': 0, 'what': 1, 'when': 2, 'where': 3, 'who': 4, 'why': 5, 'YN': 6, 'plain': 7}\n",
      "0 : 今度の週末に公園に桃の花を見に行くのはどうだい？\n",
      "0 : きょうのバスケの練習はどうだったの、ダグ？\n",
      "6 : 実際、日本で9月1日は防災の日になっています。\n",
      "7 : 来年は亥年だから、ここにある年賀状にはかわいいイノシシが描かれているんだね。\n",
      "6 : 何か読むものを買ったの？\n",
      "6 : コーヒーとケーキが欲しい人？\n",
      "6 : どんな洋服をスーツケースに入れた？\n",
      "5 : さつまいもを使ったらどうですか?\n",
      "5 : ラッキーもぼくたちといっしょに行っていい？\n",
      "2 : エレベーターの中でないならば、いつどこでこのような売り込みのしかたをするのでしょうか。\n",
      "2 : サム、いつお母さんにインタビューするつもり？\n",
      "2 : このお店はいつオープンしましたか？\n",
      "2 : モニカはいつ、アメリカでダンスを勉強することになっているの？\n",
      "4 : ぼくへのカード？\n",
      "4 : それは、だれの桃だったの？\n",
      "6 : 鯛湖のほとりにあるとても小さな町なの。\n",
      "6 : 型どおりに答えるべきではありません。\n",
      "5 : 競馬場でついに大穴を当てたのですか。\n",
      "6 : シドニーの人口は何人なの？\n",
      "0 : おふたりはどうやって知り合ったんですか？\n",
      "3 : どこに着陸したいんだい？\n",
      "3 : ここからどこへ行くの？\n",
      "7 : おいでよ。\n",
      "7 : 日曜日におもしろい映画を見たんだ。\n",
      "1 : どうしてそんなことを彼にしたのですか？\n",
      "6 : きみたちには、兄弟と姉妹が何人いるの？\n",
      "2 : エレベーターの中でないならば、いつどこでこのような売り込みのしかたをするのでしょうか。\n",
      "1 : 何て書いてあったの？\n",
      "1 : 何だい？\n",
      "0 : このウサギちゃんはどう？\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "source": [
    "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score\n",
    "print('confusion matrix = \\n', confusion_matrix(y_true=y_test, y_pred=y_pred))\n",
    "print('accuracy = ', accuracy_score(y_true=y_test, y_pred=y_pred))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "confusion matrix = \n",
      " [[ 98  10   0   0   0  11  15   2]\n",
      " [ 13  96   0   1   0   3   9   4]\n",
      " [  0   1 120   0   0   0   0   0]\n",
      " [  0   1   0  95   0   3   0   0]\n",
      " [  0   0   0   0 120   1   3   1]\n",
      " [  8  13   0   1   0  90  10   8]\n",
      " [  5   8   1   2   3  11  76   8]\n",
      " [  1   1   0   0   1   4   7  98]]\n",
      "accuracy =  0.8234683281412254\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "source": [
    "model_path = \"../models/\"\n",
    "model_name = \"typeClassify_M2.pickle\"\n",
    "modelM = DataManager(model_path)\n",
    "print(model_name)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "typeClassify_M2.pickle\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "source": [
    "modelM.save_data(model_name, lr)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "success save : ../models/typeClassify_M2.pickle\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.6.9",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.6.9 64-bit"
  },
  "interpreter": {
   "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}